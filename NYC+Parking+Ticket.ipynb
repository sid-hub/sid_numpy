{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Siddhartha\n",
    "### Creating the Spark Context through the SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"NYC_Parking_Tickets\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYC_Parking = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load('/common_folder/nyc_parking/Parking_Violations_Issued_-_Fiscal_Year_2017.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Summons Number: bigint, Plate ID: string, Registration State: string, Issue Date: timestamp, Violation Code: int, Vehicle Body Type: string, Vehicle Make: string, Violation Precinct: int, Issuer Precinct: int, Violation Time: string]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NYC_Parking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10803028"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Number of rows\n",
    "NYC_Parking.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "|Summons Number|Plate ID|Registration State|         Issue Date|Violation Code|Vehicle Body Type|Vehicle Make|Violation Precinct|Issuer Precinct|Violation Time|\n",
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "|    5092469481| GZH7067|                NY|2016-07-10 00:00:00|             7|             SUBN|       TOYOT|                 0|              0|         0143A|\n",
      "|    5092451658| GZH7067|                NY|2016-07-08 00:00:00|             7|             SUBN|       TOYOT|                 0|              0|         0400P|\n",
      "|    4006265037| FZX9232|                NY|2016-08-23 00:00:00|             5|             SUBN|        FORD|                 0|              0|         0233P|\n",
      "|    8478629828| 66623ME|                NY|2017-06-14 00:00:00|            47|             REFG|       MITSU|                14|             14|         1120A|\n",
      "|    7868300310| 37033JV|                NY|2016-11-21 00:00:00|            69|             DELV|       INTER|                13|             13|         0555P|\n",
      "|    5096917368| FZD8593|                NY|2017-06-13 00:00:00|             7|             SUBN|       ME/BE|                 0|              0|         0852P|\n",
      "|    1413609545|  X20DCM|                NJ|2016-08-03 00:00:00|            40|              SDN|       TOYOT|                71|             71|         0215A|\n",
      "|    4628525523|  326SF9|                MA|2016-12-21 00:00:00|            36|               UT|         BMW|                 0|              0|         0758A|\n",
      "|    4627113330| HCA5464|                NY|2016-11-21 00:00:00|            36|             SUBN|       DODGE|                 0|              0|         1005A|\n",
      "|    4006478550| VAD7274|                VA|2016-10-05 00:00:00|             5|               4D|         BMW|                 0|              0|         0845A|\n",
      "|    1407740258| 2513JMG|                NY|2017-01-11 00:00:00|            78|             DELV|       FRUEH|               106|            106|         0015A|\n",
      "|    8009901763| 13657MD|                NY|2016-09-27 00:00:00|            19|             DELV|       KENWO|                18|             18|         0707A|\n",
      "|    4625926610|N102911C|                NY|2016-10-27 00:00:00|            36|              VAN|        FORD|                 0|              0|         1022A|\n",
      "|    1416492320| FGR5997|                NY|2016-09-30 00:00:00|            21|              SDN|       NISSA|                44|             44|         1150A|\n",
      "|    1413656420|T672371C|                NY|2017-02-04 00:00:00|            40|             TAXI|       TOYOT|                73|             73|         0525A|\n",
      "|    7959486440| GYF2052|                NY|2016-07-07 00:00:00|            71|             4DSD|       VOLKS|               120|            120|         0645P|\n",
      "|    5093620865| AD80228|                AZ|2016-09-24 00:00:00|             7|               TK|        FORD|                 0|              0|         1122A|\n",
      "|    8480309064| 51771JW|                NY|2017-01-26 00:00:00|            64|              VAN|       INTER|                17|             17|         0256P|\n",
      "|    1416638830|  GLP367|                NY|2017-04-30 00:00:00|            20|             SUBN|       DODGE|                17|             17|         1232A|\n",
      "|    4630524241|  HJBP29|                FL|2017-02-03 00:00:00|            36|               4D|         BMW|                 0|              0|         1034A|\n",
      "+--------------+--------+------------------+-------------------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NYC_Parking.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10803028"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Dropping duplicates\n",
    "NYC_Parking=NYC_Parking.dropDuplicates()\n",
    "NYC_Parking.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10803028"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Droping null values\n",
    "NYC_Parking =NYC_Parking.dropna()\n",
    "NYC_Parking.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Creating the temporary view\n",
    "NYC_Parking.createOrReplaceTempView(\"NYC_Parking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Summons Number: bigint, Plate ID: string, Registration State: string, Issue Date: timestamp, Violation Code: int, Vehicle Body Type: string, Vehicle Make: string, Violation Precinct: int, Issuer Precinct: int, Violation Time: string]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('Select * from NYC_Parking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.show of DataFrame[Summons_Number: bigint, Plate_ID: string, Registration_State: string, Issue_Date: timestamp, Violation_Code: int, Vehicle_Body_Type: string, Vehicle_Make: string, Violation_Precinct: int, Issuer_Precinct: int, Violation_Time: string]>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Replacing the space in the field name with a underscore \n",
    "NYC_Parking= NYC_Parking.toDF(*(a.replace(' ', '_') for a in NYC_Parking.columns))\n",
    "NYC_Parking.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYC_Parking.createOrReplaceTempView(\"NYC_Parking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Summons_Number: bigint, Plate_ID: string, Registration_State: string, Issue_Date: timestamp, Violation_Code: int, Vehicle_Body_Type: string, Vehicle_Make: string, Violation_Precinct: int, Issuer_Precinct: int, Violation_Time: string]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('Select * from NYC_Parking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+\n",
      "|Year|no_of_tickets|\n",
      "+----+-------------+\n",
      "|2069|            4|\n",
      "|2068|            1|\n",
      "|2063|            2|\n",
      "|2062|            2|\n",
      "|2061|            1|\n",
      "|2060|            2|\n",
      "|2053|            1|\n",
      "|2047|            2|\n",
      "|2041|            1|\n",
      "|2036|            1|\n",
      "|2033|            2|\n",
      "|2031|            5|\n",
      "|2030|           12|\n",
      "|2029|            2|\n",
      "|2028|            8|\n",
      "|2027|           50|\n",
      "|2026|           24|\n",
      "|2025|            6|\n",
      "|2024|            3|\n",
      "|2023|            5|\n",
      "|2022|            4|\n",
      "|2021|           22|\n",
      "|2020|           22|\n",
      "|2019|          472|\n",
      "|2018|         1057|\n",
      "|2017|      5431918|\n",
      "|2016|      5368391|\n",
      "|2015|          419|\n",
      "|2014|          120|\n",
      "|2013|           70|\n",
      "|2012|           87|\n",
      "|2011|           22|\n",
      "|2010|           48|\n",
      "|2009|            3|\n",
      "|2008|            4|\n",
      "|2007|           18|\n",
      "|2006|            8|\n",
      "|2005|            1|\n",
      "|2004|            2|\n",
      "|2003|            1|\n",
      "|2002|            1|\n",
      "|2001|            2|\n",
      "|2000|          185|\n",
      "|1997|            1|\n",
      "|1996|            1|\n",
      "|1994|            1|\n",
      "|1991|            3|\n",
      "|1990|            2|\n",
      "|1985|            1|\n",
      "|1984|            1|\n",
      "|1977|            1|\n",
      "|1976|            1|\n",
      "|1974|            1|\n",
      "|1973|            2|\n",
      "|1972|            2|\n",
      "+----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tickets_year = spark.sql(\"select year(Issue_Date) as Year, count(Summons_Number) as no_of_tickets from NYC_Parking group by Year order by Year Desc\")\n",
    "tickets_year.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "### From the above results we con find that the number of tickets in 2016 and 2017 are as follows\n",
    "### We are interested in the number of tickets data for 2017\n",
    "### 2016 - 5368391 and 2017- 5431918"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5431918"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering only 2017 data\n",
    "NYC_Parking.createOrReplaceTempView(\"2017_parking\")\n",
    "NYC_Parking=spark.sql(\"select * from 2017_parking where year(TO_DATE(CAST(UNIX_TIMESTAMP(Issue_Date,'MM/dd/yyyy') AS TIMESTAMP))) = 2017 \")\n",
    "NYC_Parking.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5426657"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Removing the Blank Plates\n",
    "\n",
    "NYC_Parking=NYC_Parking[NYC_Parking.Plate_ID!='BLANKPLATE']\n",
    "NYC_Parking.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Replacing the temporary view\n",
    "NYC_Parking.createOrReplaceTempView(\"2017_parking\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|count(DISTINCT Summons_Number)|\n",
      "+------------------------------+\n",
      "|                       5426657|\n",
      "+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Q1 Find the total number of tickets for the year.\n",
    "\n",
    "Q1=spark.sql(\"Select count(distinct(Summons_Number)) from 2017_parking\")\n",
    "Q1.show()\n",
    "\n",
    "### Total number of tickets issued in 2017 is 5426657"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Q2 Find out the number of unique states from where the cars that got parking tickets came\n",
    "\n",
    "Q2 = spark.sql(\"SELECT distinct(Registration_State) from 2017_parking\")\n",
    "Q2.count()\n",
    "\n",
    "### Total 65 registration states from which the parking tickets came"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|Reg_State|Num_Tickets|\n",
      "+---------+-----------+\n",
      "|       NY|    4273951|\n",
      "|       NJ|     475825|\n",
      "|       PA|     140286|\n",
      "|       CT|      70403|\n",
      "|       FL|      69468|\n",
      "|       IN|      45525|\n",
      "|       MA|      38941|\n",
      "|       VA|      34367|\n",
      "|       MD|      30213|\n",
      "|       NC|      27152|\n",
      "|       TX|      18827|\n",
      "|       IL|      18666|\n",
      "|       GA|      17537|\n",
      "|       AZ|      12379|\n",
      "|       OH|      12281|\n",
      "|       CA|      12153|\n",
      "|       ME|      10806|\n",
      "|       99|      10794|\n",
      "|       SC|      10395|\n",
      "|       MN|      10083|\n",
      "+---------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Q2 = spark.sql(\"SELECT distinct(Registration_State) AS Reg_State, count(*) Num_Tickets from 2017_parking group by Reg_State Order by  Num_Tickets desc\")\n",
    "\n",
    "Q2.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "### There is an column entry with numeric value of 99 which needs to be replaced with NY as NY is the state from which maximum tickets were issued\n",
    "\n",
    "from pyspark.sql.functions import when,lit\n",
    "NYC_Parking=NYC_Parking.withColumn('Registration_State',when(NYC_Parking[\"Registration_State\"]==\"99\",lit('NY')).otherwise(NYC_Parking[\"Registration_State\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Replacing the temporary view after 99 is replaced by NY\n",
    "NYC_Parking.createOrReplaceTempView(\"2017_parking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|Reg_State|Num_Tickets|\n",
      "+---------+-----------+\n",
      "|       NY|    4284745|\n",
      "|       NJ|     475825|\n",
      "|       PA|     140286|\n",
      "|       CT|      70403|\n",
      "|       FL|      69468|\n",
      "|       IN|      45525|\n",
      "|       MA|      38941|\n",
      "|       VA|      34367|\n",
      "|       MD|      30213|\n",
      "|       NC|      27152|\n",
      "|       TX|      18827|\n",
      "|       IL|      18666|\n",
      "|       GA|      17537|\n",
      "|       AZ|      12379|\n",
      "|       OH|      12281|\n",
      "|       CA|      12153|\n",
      "|       ME|      10806|\n",
      "|       SC|      10395|\n",
      "|       MN|      10083|\n",
      "|       OK|       9088|\n",
      "+---------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Q2 Find out the number of unique states from where the cars that got parking tickets came after replacing 99 with NY\n",
    "\n",
    "Q2 = spark.sql(\"SELECT distinct(Registration_State) AS Reg_State, count(*) Num_Tickets from 2017_parking group by Reg_State Order by  Num_Tickets desc\")\n",
    "\n",
    "Q2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### There are 64 distinct states from which the cars got picked (after replacing 99 with NY)\n",
    "Q2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|count(DISTINCT Violation_Code)|\n",
      "+------------------------------+\n",
      "|                           100|\n",
      "+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregation Code\n",
    "### How often does each violation code occur?  (Part of Q1 Aggregation Task)\n",
    "from pyspark.sql.functions import count,desc,countDistinct\n",
    "NYC_Parking.select(countDistinct(\"Violation_Code\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aggregation Code\n",
    "### How often does each violation code occur? \n",
    "Q3 = spark.sql(\"SELECT distinct(Violation_Code) AS Violation_Code, count(*) Num_Tickets from 2017_parking group by Violation_Code Order by  Num_Tickets desc\")\n",
    "Q3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+\n",
      "|Violation_Code|Num_Tickets|\n",
      "+--------------+-----------+\n",
      "|            21|     767740|\n",
      "|            36|     662765|\n",
      "|            38|     541526|\n",
      "|            14|     476405|\n",
      "|            20|     319439|\n",
      "+--------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###  Display the frequency of the top five violation codes (Aggregation Task Q1)\n",
    "\n",
    "Q3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------+\n",
      "|Vehicle_Body_Type| Ticket|\n",
      "+-----------------+-------+\n",
      "|             SUBN|1882978|\n",
      "|             4DSD|1547063|\n",
      "|              VAN| 723796|\n",
      "|             DELV| 358924|\n",
      "|              SDN| 192927|\n",
      "+-----------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### How often does each 'vehicle body type' get a parking ticket? How about the 'vehicle make'? (Q2 in Aggregation Task)\n",
    "\n",
    "vehicleBodyType = spark.sql(\"SELECT Vehicle_Body_Type, count(*) as Ticket from 2017_parking group by Vehicle_Body_Type order by Ticket desc\")\n",
    "vehicleBodyType.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|Vehicle_Make|Ticket|\n",
      "+------------+------+\n",
      "|        FORD|636527|\n",
      "|       TOYOT|605011|\n",
      "|       HONDA|538460|\n",
      "|       NISSA|461725|\n",
      "|       CHEVR|355868|\n",
      "+------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Vehicle make (Q2 in Aggregation Task)\n",
    "\n",
    "vehicleMake = spark.sql(\"SELECT Vehicle_Make, count(*) as Ticket from 2017_parking group by Vehicle_Make order by Ticket desc\")\n",
    "vehicleMake.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------+\n",
      "|Violation_Precinct|Ticket|\n",
      "+------------------+------+\n",
      "|                 0|925395|\n",
      "|                19|274264|\n",
      "|                14|203375|\n",
      "|                 1|174620|\n",
      "|                18|169043|\n",
      "|               114|147223|\n",
      "+------------------+------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### 'Violation Precinct' (This is the precinct of the zone where the violation occurred). (Q3 Part 1 in Aggregation Task)\n",
    "### Using this, can you draw any insights for parking violations in any specific areas of the city?\n",
    "#### Here, you would have noticed that the dataframe has the'Violating Precinct' or 'Issuing Precinct' as '0'. \n",
    "### These are erroneous entries. Hence, you need to provide the records for five correct precincts. \n",
    "### (Hint: Print the top six entries after sorting.)\n",
    "Violation_Precinct = spark.sql(\"SELECT Violation_Precinct, count(*) as Ticket from 2017_parking group by Violation_Precinct order by Ticket desc\")\n",
    "Violation_Precinct.show(6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+\n",
      "|Issuer_Precinct| Ticket|\n",
      "+---------------+-------+\n",
      "|              0|1077884|\n",
      "|             19| 266790|\n",
      "|             14| 200328|\n",
      "|              1| 168630|\n",
      "|             18| 162908|\n",
      "|            114| 143900|\n",
      "+---------------+-------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### 'Issuer Precinct' (This is the precinct that issued the ticket.)(Q3 part 2 in Aggregation Task)\n",
    "\n",
    "Issue_precinct = spark.sql(\"SELECT Issuer_Precinct, count(*) as Ticket from 2017_parking  group by Issuer_Precinct order by Ticket desc\")  \n",
    "Issue_precinct.show(6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+---------+\n",
      "|Issuer_Precinct|Violation_Code|Frequency|\n",
      "+---------------+--------------+---------+\n",
      "|              0|            36|   662765|\n",
      "|              0|             7|   210175|\n",
      "|              0|            21|   125923|\n",
      "+---------------+--------------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Find the violation code frequencies for three precincts that have issued the most number of tickets. (Q4 Aggregation Task)\n",
    "\n",
    "\n",
    "violation_code_freq = spark.sql(\"select Issuer_Precinct,Violation_Code, count(*) as Frequency from 2017_parking group by Issuer_Precinct, Violation_Code order by Frequency desc\" )\n",
    "violation_code_freq.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+---------+\n",
      "|Issuer_Precinct|Violation_Code|Frequency|\n",
      "+---------------+--------------+---------+\n",
      "|              0|            36|   662765|\n",
      "|              0|             7|   210175|\n",
      "|              0|            21|   125923|\n",
      "|             18|            14|    50135|\n",
      "|             19|            46|    48422|\n",
      "|              0|             5|    48076|\n",
      "|             14|            14|    45019|\n",
      "|              1|            14|    38345|\n",
      "|             19|            38|    36332|\n",
      "|             19|            37|    36046|\n",
      "+---------------+--------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### let us look at the top 10 since the top three issuer precinct are having code 0 which we are not considering\n",
    "### So the top 3 precincts are 18, 19 and 14 (Q4 Aggregation Task)\n",
    "\n",
    "violation_code_freq.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+\n",
      "|Violation_Code|Frequency|\n",
      "+--------------+---------+\n",
      "|            14|    50135|\n",
      "|            69|    20188|\n",
      "|            47|    14105|\n",
      "+--------------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Looking for top 3 violation codes within precinct 18 having maximum tickets issued\n",
    "\n",
    "violation_code_freq_18 = spark.sql(\"select Violation_Code, count(*) as Frequency from 2017_parking where Issuer_Precinct=18 group by Violation_Code order by Frequency desc\" )\n",
    "violation_code_freq_18.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+\n",
      "|Violation_Code|Frequency|\n",
      "+--------------+---------+\n",
      "|            46|    48422|\n",
      "|            38|    36332|\n",
      "|            37|    36046|\n",
      "+--------------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Looking for top 3 violation codes within precinct 19 having maximum tickets issued\n",
    "\n",
    "violation_code_freq_19 = spark.sql(\"select Violation_Code, count(*) as Frequency from 2017_parking where Issuer_Precinct=19 group by Violation_Code order by Frequency desc\" )\n",
    "violation_code_freq_19.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+\n",
      "|Violation_Code|Frequency|\n",
      "+--------------+---------+\n",
      "|            14|    45019|\n",
      "|            69|    30453|\n",
      "|            31|    22528|\n",
      "+--------------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Looking for top 3 violation codes within precinct 14 having maximum tickets issued\n",
    "\n",
    "violation_code_freq_14 = spark.sql(\"select Violation_Code, count(*) as Frequency from 2017_parking where Issuer_Precinct=14 group by Violation_Code order by Frequency desc\" )\n",
    "violation_code_freq_14.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+\n",
      "|Violation_Code|Frequency|\n",
      "+--------------+---------+\n",
      "|            14|   124945|\n",
      "|            46|    63958|\n",
      "|            69|    53549|\n",
      "+--------------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###  The top three precincts which have issued maximum tickets on violation are precincts 18, 19 and 14\n",
    "### Do these precinct zones have an exceptionally high frequency of certain violation codes? \n",
    "### Are these codes common across precincts? \n",
    "### To find out the above, we need to look at the combined top 3 violations for precincts 18, 19 and 14(Q4 Part 2 Aggreagtion Task)\n",
    "\n",
    "combined_violation_codes =spark.sql(\"select Violation_Code, count(*) as Frequency from 2017_parking where Issuer_Precinct in (18,19,14) group by Violation_Code order by Frequency desc\")\n",
    "combined_violation_codes.show(3)\n",
    "\n",
    "#### From the below results it is clear that violation codes 14, 46 and 69 are the top 3 violation codes for the combined data\n",
    "#### As we can see from this, violation codes 14 and 69 are common across precincts 14 and 18 (this was also clear from the individual\n",
    "### zone analysis above, precinct 19 has no common violation code shared with the other two zones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|null_values|\n",
      "+-----------+\n",
      "|          0|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Should not see any null value as we had dropped the null values in the beginning\n",
    "null_values = spark.sql(\"SELECT count(*) as null_values from 2017_parking where Violation_Time is NULL\")\n",
    "null_values.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+-------------------+------+\n",
      "|Violation_Code|Issuer_Precinct|         Issue_Date|season|\n",
      "+--------------+---------------+-------------------+------+\n",
      "|            53|              5|2017-03-19 00:00:00|spring|\n",
      "|            10|             61|2017-06-20 00:00:00|summer|\n",
      "|            14|            109|2017-02-03 00:00:00|winter|\n",
      "|            36|              0|2017-05-23 00:00:00|spring|\n",
      "|            14|            109|2017-06-01 00:00:00|summer|\n",
      "|            84|             17|2017-04-05 00:00:00|spring|\n",
      "|            69|             17|2017-05-06 00:00:00|spring|\n",
      "|             7|              0|2017-03-08 00:00:00|spring|\n",
      "|            37|              9|2017-04-18 00:00:00|spring|\n",
      "|            14|              1|2017-02-21 00:00:00|winter|\n",
      "|            38|             24|2017-02-04 00:00:00|winter|\n",
      "|             7|              0|2017-01-10 00:00:00|winter|\n",
      "|            24|            112|2017-04-20 00:00:00|spring|\n",
      "|            37|              6|2017-03-29 00:00:00|spring|\n",
      "|            36|              0|2017-03-10 00:00:00|spring|\n",
      "|            36|              0|2017-06-15 00:00:00|summer|\n",
      "|            37|             62|2017-03-04 00:00:00|spring|\n",
      "|            36|              0|2017-05-10 00:00:00|spring|\n",
      "|            38|             13|2017-04-06 00:00:00|spring|\n",
      "|            21|            104|2017-05-27 00:00:00|spring|\n",
      "+--------------+---------------+-------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Finding the seasonality in the violations data(Q6)\n",
    "#### Spring runs from March 1 to May 31;\n",
    "#### Summer runs from June 1 to August 31;\n",
    "#### Fall (autumn) runs from September 1 to November 30; and.\n",
    "#### Winter runs from December 1 to February 28\n",
    "\n",
    "##### Source: Northern Meteorological Seasons from https://www.timeanddate.com\n",
    "\n",
    "\n",
    "seasonality = spark.sql(\"select Violation_Code , Issuer_Precinct,Issue_Date, case when MONTH(TO_DATE(Issue_Date, 'MM/dd/yyyy')) between 03 and 05 then 'spring' when MONTH(TO_DATE(Issue_Date, 'MM/dd/yyyy')) between 06 and 08 then 'summer' when MONTH(TO_DATE(Issue_Date, 'MM/dd/yyyy')) between 09 and 11 then 'autumn' when MONTH(TO_DATE(Issue_Date, 'MM/dd/yyyy')) in (1,2,12) then 'winter' else 'unknown' end  as season from 2017_parking\")\n",
    "seasonality.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Creating a temp view for seasonality data\n",
    "seasonality.createOrReplaceTempView(\"seasonality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|season|tickets|\n",
      "+------+-------+\n",
      "|spring|2870491|\n",
      "|winter|1702786|\n",
      "|summer| 852405|\n",
      "|autumn|    975|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Frequency of tickets for each season(Q6 Part 1)\n",
    "seasonality_freq = spark.sql(\"select season, count(*) as tickets from seasonality  group by season order by tickets desc\")\n",
    "seasonality_freq.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+\n",
      "|Violation_Code|Frequency|\n",
      "+--------------+---------+\n",
      "|            21|   402234|\n",
      "|            36|   344834|\n",
      "|            38|   270913|\n",
      "+--------------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### Then, find the three most common violations for each of these seasons.(Q6 Part2)\n",
    "\n",
    "# Top 3 violations in the Spring season (we have not filtered the data for any precincts)\n",
    "violation_spring = spark.sql(\"select Violation_Code, count(*) as Frequency from seasonality where season = 'spring' group by Violation_Code order by Frequency desc\" )\n",
    "violation_spring.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+\n",
      "|Violation_Code|Frequency|\n",
      "+--------------+---------+\n",
      "|            21|   238059|\n",
      "|            36|   221268|\n",
      "|            38|   187102|\n",
      "+--------------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top 3 violations in the winter season (we have not filtered the data for any precincts)(Q6 Part2)\n",
    "violation_winter = spark.sql(\"select Violation_Code, count(*) as Frequency from seasonality where season = 'winter' group by Violation_Code order by Frequency desc\" )\n",
    "violation_winter.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+\n",
      "|Violation_Code|Frequency|\n",
      "+--------------+---------+\n",
      "|            46|      231|\n",
      "|            21|      128|\n",
      "|            40|      115|\n",
      "+--------------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top 3 violations in the autumn season (we have not filtered the data for any precincts)(Q6 Part2)\n",
    "violation_autumn = spark.sql(\"select Violation_Code, count(*) as Frequency from seasonality where season = 'autumn' group by Violation_Code order by Frequency desc\" )\n",
    "violation_autumn.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+\n",
      "|Violation_Code|Frequency|\n",
      "+--------------+---------+\n",
      "|            21|   127319|\n",
      "|            36|    96663|\n",
      "|            38|    83503|\n",
      "+--------------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top 3 violations in the summer season (we have not filtered the data for any precincts)(Q6 Part2)\n",
    "violation_summer = spark.sql(\"select Violation_Code, count(*) as Frequency from seasonality where season = 'summer' group by Violation_Code order by Frequency desc\" )\n",
    "violation_summer.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+\n",
      "|Violation_Code|Count_Violations|\n",
      "+--------------+----------------+\n",
      "|            21|          767740|\n",
      "|            36|          662765|\n",
      "|            38|          541526|\n",
      "+--------------+----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Find the total occurrences of the three most common violation codes.(Q7 Part1)\n",
    "\n",
    "common_Violation = spark.sql(\"select Violation_Code, count(*) as Count_Violations from 2017_parking group by Violation_Code Order By Count_Violations desc\")\n",
    "common_Violation.show(3)\n",
    "\n",
    "### Total occurrences of three most common violation codes are 1972031\n",
    "### Violation Codes 21, 36 and 38 are among the top 3 in terms of the number of violations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total collection =  102440250\n",
      "Individual collection for Violation Code 21 =  42225700\n",
      "Individual collection for Violation Code 36 =  33138250\n",
      "Individual collection for Violation Code 38 =  27076300\n"
     ]
    }
   ],
   "source": [
    "### Using the information from https://www1.nyc.gov/site/finance/vehicles/services-violation-codes.page\n",
    "### find the total amount collected for the three violation codes with the maximum tickets. \n",
    "### State the code that has the highest total collection.\n",
    "### Violation Code 21 has average fine across locations is avg(65,45) = 55\n",
    "### Violation Code 36 has average fine across locations is avg(50,50) = 50\n",
    "### Violation Code 38 has average fine across locations is avg(65,35) = 50\n",
    "\n",
    "#### Using the above average fine figures for each Violation Code, the Total COllection from the three violation codes\n",
    "### for 2017 is (Q7 Part 2)\n",
    "\n",
    "print('Total collection = ',767740*55+662765*50+541526*50)\n",
    "print('Individual collection for Violation Code 21 = ',767740*55)\n",
    "print('Individual collection for Violation Code 36 = ',662765*50)\n",
    "print('Individual collection for Violation Code 38 = ',541526*50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Following are the inferences(Q7 Part 3)\n",
    "\n",
    "#### Violation Code 21 - Street Cleaning: No parking where parking is not allowed by sign, street marking or traffic control device\n",
    "### has resulted in the maximum collection \n",
    "\n",
    "##### Top 3 Violations across all areas in NY are 21, 36 amd 38 resulting in total revenue of 102440250 USD in 2017\n",
    "##### Top 3 precincts with highest violations are precincts 18, 19 and 14 \n",
    "##### The most commonly occured vilations in these 3 precincts are violation code 14 and 69\n",
    "##### Spring and Winter have the maximum number of violations followed by summer and autumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
